# -*- coding: utf-8 -*-
"""LumenAI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sXl5A1X1_TSSFSLax0ZkEj23wNL7wmL2
"""

# import osmnx
!pip install osmnx

# import os
# import osmnx as ox
# import networkx as nx
# import geopandas as gpd
# import pandas as pd
# import numpy as np
# import matplotlib.pyplot as plt
# from sklearn.preprocessing import MinMaxScaler
# from shapely.geometry import Point
# from geopy.geocoders import Nominatim
# from geopy.extra.rate_limiter import RateLimiter

# # Initialize geocoder for address parsing
# geolocator = Nominatim(user_agent="neighborhood_analysis")
# geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1)

# # Load the base neighborhood graph (from your existing code)
# def load_base_neighborhood_graph():
#     # Step 1: Download Boston's street network
#     G = ox.graph_from_place("Boston, Massachusetts, USA", network_type="all_public")

#     # Step 2: Load BPDA neighborhood boundaries
#     gdf_neighborhoods = gpd.read_file('bpda_neighborhood_boundaries/BPDA_Neighborhood_Boundaries.shp')

#     # Step 3: Simplify and prepare data
#     gdf_neighborhoods = gdf_neighborhoods.to_crs(epsg=4326)
#     gdf_neighborhoods["geometry"] = gdf_neighborhoods["geometry"].simplify(tolerance=0.001)

#     print(gdf_neighborhoods.head())

#     # Reproject the graph to match the neighborhoods' CRS
#     G = ox.project_graph(G, to_crs=gdf_neighborhoods.crs)

#     # Step 4: Get edges as a GeoDataFrame
#     gdf_edges = ox.graph_to_gdfs(G, nodes=False, edges=True, fill_edge_geometry=True)
#     gdf_edges = gdf_edges.reset_index()

#     # Step 5: Perform a spatial join to find which streets are in each neighborhood
#     gdf_joined = gpd.sjoin(gdf_edges, gdf_neighborhoods, how="inner", predicate="intersects")

#     # Step 6: Create a dictionary to map street edges to neighborhoods
#     neighborhood_connections = {}
#     for _, row in gdf_joined.iterrows():
#         neighborhood = row["name_right"]  # Replace "Name" with the correct column name
#         edge = (row["u"], row["v"])  # Edge in the street network

#         if edge not in neighborhood_connections:
#             neighborhood_connections[edge] = set()
#         neighborhood_connections[edge].add(neighborhood)

#     # Step 7: Create a graph of neighborhood connectivity
#     neighborhood_graph = nx.Graph()
#     for edge, neighborhoods in neighborhood_connections.items():
#         neighborhoods = list(neighborhoods)
#         for i in range(len(neighborhoods)):
#             for j in range(i + 1, len(neighborhoods)):
#                 neighborhood_graph.add_edge(neighborhoods[i], neighborhoods[j])

#     return neighborhood_graph, gdf_neighborhoods

# # Convert address to coordinates (for police incidents)
# def geocode_address(row):
#     try:
#         address = f"{row['block_address']}, {row['city']}, {row['zip_code']}"
#         location = geocode(address)
#         if location:
#             return pd.Series({'longitude': location.longitude, 'latitude': location.latitude})
#     except:
#         pass
#     return pd.Series({'longitude': None, 'latitude': None})

# # Load all datasets with custom column handling
# def load_datasets():
#     datasets = {}

#     # Police incidents - has block_address, city, zip_code
#     police_incidents = pd.read_csv('police_incidents.csv')
#     # Geocode addresses (this may take a while)
#     coords = police_incidents.apply(geocode_address, axis=1)
#     police_incidents = pd.concat([police_incidents, coords], axis=1).dropna(subset=['latitude', 'longitude'])
#     datasets['police_incidents'] = gpd.GeoDataFrame(
#         police_incidents,
#         geometry=gpd.points_from_xy(police_incidents.longitude, police_incidents.latitude),
#         crs='EPSG:4326'
#     )

#     # Lighting - has Lat and Long
#     lighting = pd.read_csv('streetlight_locations.csv')
#     datasets['lighting'] = gpd.GeoDataFrame(
#         lighting,
#         geometry=gpd.points_from_xy(lighting.Long, lighting.Lat),
#         crs='EPSG:4326'
#     )

#     # Schools - has POINT_X and POINT_Y
#     schools = pd.read_csv('public_schools.csv')
#     datasets['schools'] = gpd.GeoDataFrame(
#         schools,
#         geometry=gpd.points_from_xy(schools.POINT_X, schools.POINT_Y),
#         crs='EPSG:4326'
#     )

#     # Police stations - has POINT_X and POINT_Y
#     police_stations = pd.read_csv('police_stations.csv')
#     datasets['police_stations'] = gpd.GeoDataFrame(
#         police_stations,
#         geometry=gpd.points_from_xy(police_stations.POINT_X, police_stations.POINT_Y),
#         crs='EPSG:4326'
#     )

#     # Shootings - has NEIGHBORHOOD (no coordinates needed)
#     datasets['shootings'] = pd.read_csv('shootings.csv')

#     # Census data - has neighborhoods
#     datasets['census'] = pd.read_csv('census.csv')

#     return datasets

# # Calculate neighborhood statistics from datasets
# def calculate_neighborhood_stats(gdf_neighborhoods, datasets):
#     stats = {}
#     neighborhood_names = gdf_neighborhoods['name'].unique()

#     for neighborhood in neighborhood_names:
#         stats[neighborhood] = {}
#         geom = gdf_neighborhoods[gdf_neighborhoods['name'] == neighborhood].geometry.iloc[0]

#         # Police incidents count
#         if 'police_incidents' in datasets:
#             incidents_in_neighborhood = datasets['police_incidents'][datasets['police_incidents'].geometry.intersects(geom)]
#             stats[neighborhood]['police_incidents_count'] = len(incidents_in_neighborhood)

#         # Lighting count
#         if 'lighting' in datasets:
#             lights_in_neighborhood = datasets['lighting'][datasets['lighting'].geometry.intersects(geom)]
#             stats[neighborhood]['lighting_count'] = len(lights_in_neighborhood)

#         # Schools count
#         if 'schools' in datasets:
#             schools_in_neighborhood = datasets['schools'][datasets['schools'].geometry.intersects(geom)]
#             stats[neighborhood]['schools_count'] = len(schools_in_neighborhood)

#         # Police stations count
#         if 'police_stations' in datasets:
#             stations_in_neighborhood = datasets['police_stations'][datasets['police_stations'].geometry.intersects(geom)]
#             stats[neighborhood]['police_stations_count'] = len(stations_in_neighborhood)

#         # Shootings count (using NEIGHBORHOOD column)
#         if 'shootings' in datasets and 'NEIGHBORHOOD' in datasets['shootings'].columns:
#             shootings_in_neighborhood = datasets['shootings'][datasets['shootings']['NEIGHBORHOOD'].str.lower() == neighborhood.lower()]
#             stats[neighborhood]['shootings_count'] = len(shootings_in_neighborhood)

#         # Census data (using neighborhood column)
#         if 'census' in datasets and 'neighborhood' in datasets['census'].columns:
#             census_data = datasets['census'][datasets['census']['neighborhood'].str.lower() == neighborhood.lower()]
#             if 'population' in census_data.columns:
#                 stats[neighborhood]['population'] = census_data['population'].sum()
#             if 'median_income' in census_data.columns:
#                 stats[neighborhood]['median_income'] = census_data['median_income'].mean()

#     return stats

# # Calculate edge weights based on neighborhood statistics
# def calculate_edge_weights(neighborhood_graph, neighborhood_stats):
#     # Normalize statistics
#     scaler = MinMaxScaler()
#     stats_df = pd.DataFrame.from_dict(neighborhood_stats, orient='index')

#     # Define weights for each factor (sum to 1)
#     weights = {
#         'police_incidents_count': -0.25,  # Negative because more incidents is worse
#         'lighting_count': 0.05,
#         'schools_count': 0.20,
#         'police_stations_count': 0.15,
#         'shootings_count': -0.15,  # Negative because more shootings is worse
#         'population': 0.05,
#         'median_income': 0.20
#     }

#     # Normalize and calculate weighted score
#     stats_df['composite_score'] = 0
#     for factor, weight in weights.items():
#         if factor in stats_df.columns:
#             # Handle missing data
#             if stats_df[factor].isnull().all():
#                 continue

#             # Normalize (invert if weight is negative)
#             if weight < 0:
#                 stats_df[factor] = 1 - scaler.fit_transform(stats_df[[factor]].fillna(0))
#             else:
#                 stats_df[factor] = scaler.fit_transform(stats_df[[factor]].fillna(0))

#             stats_df['composite_score'] += weight * stats_df[factor]

#     # Scale final score to 0-1 range
#     stats_df['composite_score'] = scaler.fit_transform(stats_df[['composite_score']])

#     # Update edge weights in the graph
#     for u, v in neighborhood_graph.edges():
#         if u in stats_df.index and v in stats_df.index:
#             # Weight is the average of the two neighborhoods' composite scores
#             weight = (stats_df.loc[u, 'composite_score'] + stats_df.loc[v, 'composite_score']) / 2
#             neighborhood_graph[u][v]['weight'] = weight

#     return neighborhood_graph, stats_df

# # Main function
# def main():
#     print("Loading base neighborhood graph...")
#     neighborhood_graph, gdf_neighborhoods = load_base_neighborhood_graph()

#     print(gdf_neighborhoods.head())

#     print("Loading datasets...")
#     datasets = load_datasets()

#     print("Calculating neighborhood statistics...")
#     neighborhood_stats = calculate_neighborhood_stats(gdf_neighborhoods, datasets)

#     print("Calculating edge weights...")
#     weighted_graph, neighborhood_stats_df = calculate_edge_weights(neighborhood_graph.copy(), neighborhood_stats)

#     # Save results
#     print("Saving results...")
#     nx.write_gml(weighted_graph, "weighted_neighborhood_graph.gml")
#     neighborhood_stats_df.to_csv("neighborhood_statistics.csv")

#     # Visualization
#     print("Creating visualization...")
#     plt.figure(figsize=(14, 12))
#     pos = nx.spring_layout(weighted_graph, seed=42)

#     # Draw edges with width proportional to weight
#     edges = weighted_graph.edges(data=True)
#     edge_widths = [d['weight']*3 for (u, v, d) in edges]

#     nx.draw_networkx_edges(weighted_graph, pos, width=edge_widths, edge_color="gray", alpha=0.7)
#     nx.draw_networkx_nodes(weighted_graph, pos, node_size=700, node_color="lightblue", alpha=0.9)
#     nx.draw_networkx_labels(weighted_graph, pos, font_size=10, font_weight="bold")

#     plt.title("Weighted Neighborhood Connectivity Graph\n(Edge width represents connection strength)", fontsize=14)
#     plt.axis('off')

#     # Print top connections
#     print("\nTop neighborhood connections by weight:")
#     sorted_edges = sorted(edges, key=lambda x: x[2]['weight'], reverse=True)
#     for u, v, d in sorted_edges:
#         print(f"{u: <25} -- {v: <25}: {d['weight']:.3f}")

#     # Add colorbar for weights
#     sm = plt.cm.ScalarMappable(cmap='viridis', norm=plt.Normalize(vmin=0, vmax=1))
#     sm._A = []
#     cbar = plt.colorbar(sm, shrink=0.7)
#     cbar.set_label('Connection Weight', rotation=270, labelpad=15)

#     plt.tight_layout()
#     plt.savefig("neighborhood_connectivity.png", dpi=300)
#     plt.show()



# if __name__ == "__main__":
#     main()

import geopandas as gpd
import matplotlib.pyplot as plt

# Replace with your actual path to the .shp file
shapefile_path = "streets.shp"

# Load the shapefile with GeoPandas
streets_gdf = gpd.read_file(shapefile_path)

# Plot the data
fig, ax = plt.subplots(figsize=(10, 10))
streets_gdf.plot(ax=ax, linewidth=0.5, color="black")

# Add title and axis off
ax.set_title("Boston Street Map", fontsize=15)
ax.axis("off")

# Show the plot
plt.tight_layout()
plt.show()

import networkx as nx
import geopandas as gpd
import matplotlib.pyplot as plt
from shapely.geometry import LineString
import pandas as pd
from shapely.geometry import Point

G = nx.Graph()

def load_datasets():
    datasets = {}

    police_stations = pd.read_csv('police_stations.csv')
    datasets['police_stations'] = gpd.GeoDataFrame(
        police_stations,
        geometry=gpd.points_from_xy(police_stations.POINT_X, police_stations.POINT_Y),
        crs='EPSG:4326'
    )

    public_schools = pd.read_csv('public_schools.csv')
    datasets['public_schools'] = gpd.GeoDataFrame(
        public_schools,
        geometry=gpd.points_from_xy(public_schools.POINT_X, public_schools.POINT_Y),
        crs='EPSG:4326'
    )

    streetlights = pd.read_csv('streetlight_locations.csv')
    datasets['streetlights'] = gpd.GeoDataFrame(
        streetlights,
        geometry=gpd.points_from_xy(streetlights.Long, streetlights.Lat),
        crs='EPSG:4326'
    )

    shootings = pd.read_csv('shootings.csv')
    datasets['shootings'] = gpd.GeoDataFrame(
        shootings,
        geometry=gpd.points_from_xy(shootings.lng, shootings.lat),
        crs='EPSG:4326'
    )

    return datasets


print('Loading datasets...')
datasets = load_datasets()

from shapely.geometry import box

def min_distance_filtered(x, gdf, buffer=500):  # buffer in meters
    minx, miny, maxx, maxy = x.bounds

    # Expand bounding box
    bbox = box(minx - buffer, miny - buffer, maxx + buffer, maxy + buffer)

    # Filter geometries within that box
    nearby = gdf[gdf.geometry.intersects(bbox)]

    if not nearby.empty:
        return nearby.distance(x)
    else:
        # fallback to full scan
        return gdf.distance(x)

def compute_metrics(df_nodes, gdf_police, gdf_schools, gdf_lights, gdf_shootings):

  df_metrics = pd.DataFrame(index=df_nodes.index)
  df_nodes = df_nodes.to_crs(epsg=4326)

  df_metrics['centroid_lonlat'] = df_nodes.geometry
  print(df_nodes.geometry)
  df_metrics['point1_long'] = df_nodes.geometry[0]
  df_metrics['lon'] = df_nodes.geometry[1]

  print(df_metrics.head(5))

  radius = 1609.34 * 2  # 10 miles in meters

  # Ensure all GeoDataFrames are in projected CRS (e.g., EPSG:3857 or 32619 for distance in meters)
  df_nodes = df_nodes.to_crs(epsg=3857)
  gdf_police = gdf_police.to_crs(epsg=3857)
  gdf_schools = gdf_schools.to_crs(epsg=3857)
  gdf_lights = gdf_lights.to_crs(epsg=3857)
  gdf_shootings = gdf_shootings.to_crs(epsg=3857)

  df_nodes = df_nodes[~df_nodes.geometry.isna()]

  df_metrics['centroid_3857'] = df_nodes.geometry.centroid
  print(df_metrics.head(5))

  # Distance to nearest
  df_metrics['dist_police_stations'] = df_nodes.geometry.apply(
      lambda x: gdf_police.geometry.distance(x).min())
  df_metrics['dist_public_schools'] = df_nodes.geometry.apply(
      lambda x: gdf_schools.distance(x).min())
  # df_metrics['dist_streetlights'] = df_nodes.geometry.apply(
  #     lambda x: gdf_lights.distance(x).min())

  # Apply with filter to each node
  df_metrics['dist_streetlights'] = df_nodes.geometry.apply(
      lambda x: min_distance_filtered(x, gdf_lights).min())

  df_metrics['dist_shootings'] = df_nodes.geometry.apply(
      lambda x: gdf_shootings.distance(x).min())

  # Count within 10 miles
  df_metrics['num_police_stations'] = df_nodes.geometry.apply(
      lambda x: gdf_police.distance(x).lt(radius).sum())
  df_metrics['num_public_schools'] = df_nodes.geometry.apply(
      lambda x: gdf_schools.distance(x).lt(radius).sum())
  df_metrics['num_streetlights'] = df_nodes.geometry.apply(
      lambda x: min_distance_filtered(x, gdf_lights).sum())
  df_metrics['num_shootings'] = df_nodes.geometry.apply(
      lambda x: gdf_shootings.distance(x).lt(radius).sum())

  return df_metrics

streets_gdf.to_crs(epsg=4326, inplace=True)
# Build graph from shapefile geometry
print('Computing metrics...')

df_metrics = compute_metrics(streets_gdf, datasets['police_stations'], datasets['public_schools'], datasets['streetlights'], datasets['shootings'])
print(df_metrics.head(5))

df_metrics.to_csv('metrics.csv')

def load_metrics():
  metrics = pd.read_csv('metrics.csv')
  return metrics

print('Loading metrics...')
metrics = load_metrics()
print(metrics.dtypes)

# Check if lat/lon from metrics matches df_nodes geometry coords for some samples
for idx in metrics.index[:5]:
    metric_lat = metrics.loc[idx, 'lat']
    metric_lon = metrics.loc[idx, 'lon']
    node_point = streets_gdf.loc[idx].geometry
    print(f"Index {idx}: metrics lat/lon = ({metric_lat}, {metric_lon}), node geometry = ({node_point.centroid.y}, {node_point.centroid.x})")

# Function to compute a custom safety index for an edge
def compute_safety_index(point1, point2):
  if count < 10:
    print(point1, point2)
    print(metrics['lat'].head(5))
    print(point1[1])
    print(metrics[metrics['lat'] == point1[1]])

  return 0.2

print('Assigning weights...')
count = 0
for idx, row in streets_gdf.iterrows():
    geom = row.geometry
    if count < 10:
      print(geom)
    if isinstance(geom, LineString):
        coords = list(geom.coords)
        for i in range(len(coords) - 1):
            point1 = coords[i]
            point2 = coords[i + 1]
            count += 1
            weight = compute_safety_index(point1, point2)
            G.add_edge(point1, point2, weight=weight)

# Output number of nodes and edges
print(f"Graph created with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges.")

edge_geoms = []
for u, v, data in G.edges(data=True):
    edge_geoms.append(LineString([u, v]))

# Create a GeoDataFrame from the edges
edge_gdf = gpd.GeoDataFrame(geometry=edge_geoms, crs=streets_gdf.crs)
weights = [G[u][v]['weight'] for u, v in G.edges()]
edge_gdf["weight"] = weights

# First plot the base layer (e.g., city streets)
base = streets_gdf.plot(figsize=(12, 12), color='lightgray', linewidth=0.3)

# Then overlay the edge graph with color based on weights
edge_gdf.plot(
    ax=base,
    column="weight",
    cmap="RdYlGn_r",
    linewidth=1.5,
    legend=True,
)

plt.title("Street Safety Visualization by Edge Weight")
plt.axis("off")
plt.show()

